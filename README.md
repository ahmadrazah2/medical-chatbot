
# ğŸ¥ Medical Chatbot using RAG (LangChain + Mistral)

A **Retrieval-Augmented Generation (RAG)** based **medical chatbot** that answers medical questions using a **medical book as knowledge base**.  
The system uses **LangChain**, **ChromaDB**, **multilingual embeddings**, and a **local Mistral 7B Instruct GGUF model** for offline, fast, and reliable inference.

---

## ğŸš€ Features

- ğŸ” Retrieval-Augmented Generation (RAG)
- ğŸ§  Local LLM inference (no paid API required)
- ğŸ“š Medical bookâ€“based knowledge retrieval
- ğŸ’¾ Persistent vector database (ChromaDB)
- âš¡ Optimized GGUF model (Q6_K)

---

## ğŸ§  Technology Stack

| Component | Tool |
|---------|------|
| LLM | Mistral-7B-Instruct (GGUF) |
| Framework | LangChain |
| Embeddings | intfloat/multilingual-e5-large |
| Vector Store | ChromaDB |
| Language | Python |
| Deployment | Local (Offline) |

---

## ğŸ“ Project Structure

```

medical-chatbot-rag/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py
â”‚   â”œâ”€â”€ prompt.py
â”‚   â””â”€â”€ **init**.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ medical_book/      # Medical book text files
â”‚
â”œâ”€â”€ chroma_db/             # Persisted vector database
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral_models/
â”‚       â””â”€â”€ 7B-Instruct-v0.3-GGUF/
â”‚           â””â”€â”€ Mistral-7B-Instruct-v0.3.Q6_K.gguf
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ¤– Local LLM Configuration

The chatbot uses a **local GGUF model**:

```python
from pathlib import Path

llm_path = (
    Path.home()
    / "mistral_models"
    / "7B-Instruct-v0.3-GGUF"
    / "Mistral-7B-Instruct-v0.3.Q6_K.gguf"
)
````

**Recommended RAM:** 16 GB
**Context length:** 4096 tokens

---

## ğŸ” Embeddings & Chunking

### Embedding Model

```python
intfloat/multilingual-e5-large
```

### Text Chunking

* `chunk_size = 500`
* `chunk_overlap = 20`

```python
extracted_data: List[Document],
chunk_size: int = 500,
chunk_overlap: int = 20,
```

---

## ğŸ—ƒï¸ Vector Store (ChromaDB)

Documents are embedded and stored persistently:

```python
vectorstore = Chroma.from_documents(
    documents=text_chunks,
    embedding=embeddings,
    persist_directory=chroma_persist_directory,
)

vectorstore.persist()
```

âœ” Embeddings are generated **once** and reused on every run.

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Create Environment

```bash
conda create -n medicalbot python=3.10 -y
conda activate medicalbot
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Run the Chatbot

```bash
python app.py
```

If using Flask UI:

```
http://127.0.0.1:5000
```

---

## ğŸ”„ RAG Pipeline Workflow

1. Load medical book documents
2. Split text into overlapping chunks
3. Generate multilingual embeddings
4. Store vectors in ChromaDB
5. Retrieve relevant context for user query
6. Generate final answer using Mistral 7B

---

## ğŸ§ª Tips for Better Results

* Increase retrieval `k` for deeper context
* Improve prompt to force **context-only answers**
* Use higher chunk size for long explanations
* Use Korean prompt template for Korean queries

---

## ğŸ“Œ Limitations

* Not a replacement for professional medical advice
* Depends on quality of medical book data
* Local inference speed depends on hardware

---

## ğŸ“œ License

This project is intended for **educational and research purposes**.

---

## ğŸ‘¤ Author

**Ahmad Raza**
AI & Computer Vision Engineer
Research Focus: RAG Systems, LLMs, Medical AI

ğŸ”— GitHub: [https://github.com/ahmadrazah2](https://github.com/ahmadrazah2)

```


Just tell me ğŸ‘
```
